{"cells":[{"metadata":{"id":"nYITrh5b5fEm","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport librosa\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport librosa.display\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.autograd import Variable\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom torchvision import datasets, transforms\nfrom tqdm import tqdm_notebook as tqdm\nimport os\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn import preprocessing\n\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\n\n\n#from keras.datasets import mnist","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.chdir(\"/kaggle/input/ravdess/Ravdess\")","execution_count":null,"outputs":[]},{"metadata":{"id":"VjY7wxtQ4x2S","trusted":true},"cell_type":"code","source":"def load_and_get_audio_data(path_to_data_for_audio):\n    \"\"\"\n    \n    path_to_data_for_audio: Path to the Audio_Speech_Actors_01 folder.\n    output: Pandas Dataframe\n    \n    source: https://github.com/mkosaka1/Speech_Emotion_Recognition\n    \"\"\"\n\n    actor_folders = os.listdir(path_to_data_for_audio)\n\n\n    emotion = []\n    gender = []\n    actor = []\n    file_path = []\n    for i in actor_folders:\n        filename = os.listdir(path_to_data_for_audio + i) #iterate over Actor folders\n        for f in filename: # go through files in Actor folder\n            part = f.split('.')[0].split('-')\n            emotion.append(int(part[2]))\n            actor.append(int(part[6]))\n            bg = int(part[6])\n            if bg%2 == 0:\n                bg = \"female\"\n            else:\n                bg = \"male\"\n            gender.append(bg)\n            file_path.append(path_to_data_for_audio + i + '/' + f)\n    # PUT EXTRACTED LABELS WITH FILEPATH INTO DATAFRAME\n    audioDeep_df = pd.DataFrame(emotion)\n    audioDeep_df = audioDeep_df.replace({1:'neutral', 2:'calm', 3:'happy', 4:'sad', 5:'angry', 6:'fear', 7:'disgust', 8:'surprise'})\n    audioDeep_df = pd.concat([pd.DataFrame(gender),audioDeep_df,pd.DataFrame(actor)],axis=1)\n    audioDeep_df.columns = ['gender','emotion','actor']\n    audioDeep_df = pd.concat([audioDeep_df,pd.DataFrame(file_path, columns = ['path'])],axis=1)\n    \n    \n   # ITERATE OVER ALL AUDIO FILES AND EXTRACT LOG MEL SPECTROGRAM MEAN VALUES INTO DF FOR MODELING \n    df = pd.DataFrame(columns=['mel_spectrogram'])\n\n    counter=0\n\n    for index,path in enumerate(audioDeep_df.path):\n        X, sample_rate = librosa.load(path, res_type='kaiser_fast',duration=3,sr=44100,offset=0.5)\n\n        spectrogram = librosa.feature.melspectrogram(y=X, sr=sample_rate, n_mels=128,fmax=sample_rate/2)\n        db_spec = librosa.power_to_db(spectrogram)\n\n        df.loc[counter] = [db_spec]\n        counter=counter+1\n    \n    \n    \n    audioDeep_df = pd.concat([audioDeep_df,pd.DataFrame(df['mel_spectrogram'].values.tolist())],axis=1)\n    \n    \"\"\"\n   # spectrogram as image files\n    df = pd.DataFrame(columns=['mel_spectrogram_img'])\n\n    counter=0\n\n    for index,path in enumerate(audioDeep_df.path):\n        X, sample_rate = librosa.load(path, res_type='kaiser_fast',duration=3,sr=44100,offset=0.5)\n        \n        spectrogram = librosa.feature.melspectrogram(y=X, sr=sample_rate, n_mels=128,fmax=sample_rate/2)\n        db_spec = librosa.power_to_db(spectrogram)\n        db_spec = spec_to_image(db_spec)\n        \n        df.loc[counter] = [db_spec]\n        counter=counter+1\n    \n    \n    \n    audioDeep_df = pd.concat([audioDeep_df,pd.DataFrame(df['mel_spectrogram_img'].values.tolist())],axis=1)\n    \"\"\"\n    \n    return audioDeep_df","execution_count":null,"outputs":[]},{"metadata":{"id":"P_j6q4X05EFG","outputId":"9518c0f6-6aac-4536-9173-6f71845a9ec0","trusted":true},"cell_type":"code","source":"#path_to_data_for_audio = \"Data/AudioEmotion/AudioEmotion/Audio_Speech_Actors_01-24/\"\npath_to_data_for_audio = './Audio Emotion/'\naudioDeep = load_and_get_audio_data(path_to_data_for_audio)\naudioDeep.columns= ['gender', 'emotion', 'actor','path',\"mel_spectrogram\"]\naudioDeep.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"E6QsH8NOx8ip","trusted":true},"cell_type":"code","source":"def spec_to_image(spec, eps=1e-6):\n  mean = spec.mean()\n  std = spec.std()\n  spec_norm = (spec - mean) / (std + eps)\n  spec_min, spec_max = spec_norm.min(), spec_norm.max()\n  spec_scaled = 255 * (spec_norm - spec_min) / (spec_max - spec_min)\n  spec_scaled = spec_scaled.astype(np.uint8)\n  return spec_scaled","execution_count":null,"outputs":[]},{"metadata":{"id":"KEfV4812yEEk","trusted":true},"cell_type":"code","source":"def get_melspectrogram_db(file_path, sr=44100, n_fft=2048, hop_length=512, n_mels=128, fmin=20, fmax=8300, top_db=80):\n  wav,sr = librosa.load(file_path,sr=sr,duration=5) # i added duration, and changed sr\n  if wav.shape[0]<5*sr:\n    wav=np.pad(wav,int(np.ceil((5*sr-wav.shape[0])/2)),mode='reflect')\n  else:\n    wav=wav[:5*sr]\n  spec=librosa.feature.melspectrogram(wav, sr=sr, n_fft=n_fft,\n              hop_length=hop_length,n_mels=n_mels,fmin=fmin,fmax=fmax)\n  spec_db=librosa.power_to_db(spec,top_db=top_db)\n  return spec_db","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"audioDeep[\"mel_spectrogram\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# NORMALIZE DATA\n#mean = np.mean(audioDeep[\"mel_spectrogram\"], axis=0)\n#mean = np.mean(audioDeep[\"mel_spectrogram\"])\n#std = np.std(audioDeep[\"mel_spectrogram\"])\n#audioDeep[\"mel_spectrogram\"] = (audioDeep[\"mel_spectrogram\"] - mean)/std\n#X_test = (X_test - mean)/std\n#audioDeep[\"mel_spectrogram\"]","execution_count":null,"outputs":[]},{"metadata":{"id":"QJ3oR-ev5bhw","outputId":"83669334-2508-4ee4-d881-7de963453c4f","trusted":true},"cell_type":"code","source":"# Split training data into valid and test.\ntrain_dataz, test_dataz = train_test_split(audioDeep, test_size=0.2, random_state=0,\n                               stratify=audioDeep[['emotion','gender','actor']])\n\ntrain_dataz, valid_dataz = train_test_split(train_dataz, test_size=0.25, random_state=0)\n\ntrain = train_dataz\nvalid = valid_dataz\ntest = test_dataz\n\n\nprint(f'Number of training examples: {len(train_dataz)}')\nprint(f'Number of validation examples: {len(valid_dataz)}')\nprint(f'Number of testing examples: {len(test_dataz)}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split image paths from labels\n#(train_X, train_y), (test_X, test_y) = mnist.load_data()\n#(spec_to_image(get_melspectrogram_db(filename, sr)), cmap='viridis')\n\n#x_train = train[[\"mel_spectrogram\"]]**2 # Attempts to normalize, multiplied by power. Abs() is another attempt.\nx_train = train[[\"mel_spectrogram\"]]\ny_train = train[[\"gender\", \"emotion\",\"actor\"]]\n\n#x_val = valid[[\"mel_spectrogram\"]]**2\nx_val = valid[[\"mel_spectrogram\"]]\ny_val = valid[[\"gender\", \"emotion\",\"actor\"]]\n\n#x_test = test[[\"mel_spectrogram\"]]**2\nx_test = test[[\"mel_spectrogram\"]]\ny_test = test[[\"gender\", \"emotion\",\"actor\"]]\n\n\n#x_train = x_train.load_data\n#imgTest = spec_to_image(get_melspectrogram_db(str(x_train.iloc[0]['path']), 44100))\nimgTest = spec_to_image((x_train.iloc[0]['mel_spectrogram']))\n\n\n#x_train\nlibrosa.display.specshow(imgTest)\n\n#df_test.iloc[0]\n\n#librosa.display.specshow(spec_to_image(get_melspectrogram_db(x_train[0], 44100)), cmap='viridis')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Convert values to image, to array, insert padding and then convert to tensors\n\nx_traindata = []\nx_valdata = []\nx_testdata = []\n\n# For Training data\nfor item in range(len(x_train)):\n    x_traindata.append(spec_to_image(x_train.iloc[item]['mel_spectrogram'])[np.newaxis,...])\n\nfor item in range(len(x_traindata)):\n\n    print(len(x_traindata[item][0]))\n    # Pad null values\n    x_traindata[item] = np.pad(x_traindata[item], ((0,0), (0,0), (0, 259 - len(x_traindata[item][0][0])))\n    , 'constant', constant_values=0)\n    #Pad 128 to equal 259 values\n    x_traindata[item] = np.pad(x_traindata[item], ((0,0), (0,259 - len(x_traindata[item][0])), (0, 0))\n    , 'constant', constant_values=0)\n    print(x_traindata[item].shape)\n\n# For Validation data\nfor item in range(len(x_val)):\n    x_valdata.append(spec_to_image(x_val.iloc[item]['mel_spectrogram'])[np.newaxis,...])\n\nfor item in range(len(x_valdata)):\n\n    print(len(x_valdata[item][0][0]))\n    # Pad null values\n    x_valdata[item] = np.pad(x_valdata[item], ((0,0), (0,0), (0, 259 - len(x_valdata[item][0][0])))\n    , 'constant', constant_values=0)\n    #Pad 128 to equal 259 values\n    x_valdata[item] = np.pad(x_valdata[item], ((0,0), (0,259 - len(x_valdata[item][0])), (0, 0))\n    , 'constant', constant_values=0)\n    print(x_valdata[item].shape)\n\n# For Testing data\nfor item in range(len(x_test)):\n    x_testdata.append(spec_to_image(x_test.iloc[item]['mel_spectrogram'])[np.newaxis,...])\n\nfor item in range(len(x_testdata)):\n\n    print(len(x_testdata[item][0][0]))\n    # Pad null values\n    x_testdata[item] = np.pad(x_testdata[item], ((0,0), (0,0), (0, 259 - len(x_testdata[item][0][0])))\n    , 'constant', constant_values=0)\n    #Pad 128 to equal 259 values\n    x_testdata[item] = np.pad(x_testdata[item], ((0,0), (0,259 - len(x_testdata[item][0])), (0, 0))\n    , 'constant', constant_values=0)\n    print(x_testdata[item].shape)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#y_train['emotion'].values\n\ny_traindata = []\ny_valdata = []\ny_testdata = []\n\n# For Training data\nfor item in range(len(y_train)):\n    y_traindata.append((y_train.iloc[item]['emotion']))\n\n# For Validation data\nfor item in range(len(y_val)):\n    y_valdata.append((y_val.iloc[item]['emotion']))\n\n# For Testing data\nfor item in range(len(y_test)):\n    y_testdata.append((y_test.iloc[item]['emotion']))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"le = preprocessing.LabelEncoder()\ny_traindatatargets = le.fit_transform(y_traindata)\ny_valdatatargets = le.fit_transform(y_valdata)\ny_testdatatargets = le.fit_transform(y_testdata)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### LOAD THE DATA INTO TENSORS ###\n#x_trainz = torch.tensor(x_traindata,dtype=torch.float32)/864. #864 #255 might be good?\nx_trainz = torch.tensor(x_traindata,dtype=torch.float32)\ny_trainz = torch.tensor(y_traindatatargets,dtype=torch.long)\n\n#x_valz = torch.tensor(x_valdata,dtype=torch.float32)/864.\nx_valz = torch.tensor(x_valdata,dtype=torch.float32)\ny_valz = torch.tensor(y_valdatatargets,dtype=torch.long)\n\n#x_testz = torch.tensor(x_testdata,dtype=torch.float32)/864.\nx_testz = torch.tensor(x_testdata,dtype=torch.float32)\ny_testz = torch.tensor(y_testdatatargets,dtype=torch.long)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## AUTOENCODEER"},{"metadata":{"trusted":true},"cell_type":"code","source":"### ITS AUTOENCODEER TIME!!!","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Commented out for CNN\n#x_trainz = x_trainz.reshape([-1,128*259])\n#x_valz = x_trainz.reshape([-1,128*259])\n#x_testz = x_testz.reshape([-1,128*259])\n\n#x_trainz = x_trainz.reshape([-1,128,259])\n#x_valz = x_trainz.reshape([-1,128,259])\n#x_testz = x_testz.reshape([-1,128,259])\n\n\n#x_trainz = x_trainz.reshape([128,128])\n#x_valz = x_trainz.reshape([128,128])\n#x_testz = x_testz.reshape([128,128])\n\n#x_trainz = x_trainz.resize([864,1,128,128])\n#x_valz = x_trainz.resize([288,1,128,128])\n#x_testz = x_testz.resize([288,1,128,128])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_testz.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_testz.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_trainz.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_trainz.shape # This should be 512, but it is 864 ???","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# [864, 1, 259, 259]\n# batch_size, depth, height, width, channels\n\nEncoder = nn.Sequential(nn.Conv2d(1, 8, 3, stride=1, padding=0),\n                        nn.ReLU(),\n                        nn.Conv2d(8, 4, 3, stride=1, padding=0),\n                        #nn.Linear()\n                       )\n\n\nDecoder = nn.Sequential(nn.ConvTranspose2d(4, 8, 3, stride=1, padding=0),\n                        nn.ReLU(),\n                        nn.ConvTranspose2d(8, 1, 3, stride=1, padding=0),\n                        nn.Sigmoid()\n                       )\n\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nEncoder = Encoder.to(device)\nDecoder = Decoder.to(device)\n\n\noptimizer = optim.Adam(list(Encoder.parameters()) + list(Decoder.parameters()))\nloss_function = nn.MSELoss()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# RUN THE FIRST AUTO MODEL\n\n\nrunning_loss = []\nfor iteration in range(10000**100):\n  \n    Encoder.train()\n    Decoder.train()\n    \n    \n    # # 1%, 25%, 50%, 75%, 100%\n    # #Fewer numbers of labels\n    percentage_of_label = .01\n    number_of_samples = int(len(y_trainz) * percentage_of_label)\n\n    random_indexes = np.random.choice(range(0,len(y_trainz)), number_of_samples)\n    \n    \n    \n\n    #random_indexes = np.random.choice(range(0,len(y_trainz)),512) # This was the old code\n    \n    #print(random_indexes.shape)\n    #print(x_trainz.shape)\n    #print(y_trainz.shape)\n\n    x_traina = Variable(np.take(x_trainz,random_indexes,0)).to(device)\n    \n    #print(x_traina.shape) # [864, 1, 128, 259]\n    \n    optimizer.zero_grad()\n\n    latent_variable = Encoder(x_traina)\n    recon_input = Decoder(latent_variable)\n    \n    # the bad evil fix is here.\n    #recon_input = recon_input.reshape([512, 1, 128, 259]) # evil experiment, dont do it!!! This is bad\n    #target = torch.zeros(512, 1, 128, 259)\n    #source = recon_input\n    #target[:, :, :121 , :257] = source\n    \n    #recon_input = target\n    \n    #512, 1, 121, 257\n    \n    # We are trying to use this fix because\n    \n    \n    #print(latent_variable.shape)\n    #print(recon_input.shape) # 512, 1, 121, 257\n    loss = loss_function(recon_input, x_traina)\n\n    loss.backward()\n  \n    optimizer.step()\n\n    running_loss.append(loss.item())\n\n\n    if iteration%1000 == 0:\n        r_loss = sum(running_loss)/len(running_loss)\n        #print (r_loss/len(x_trainz)) # Revised loss as a percentage.\n        print (r_loss)\n        running_loss = []\n        \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#orginal was 128*259, padded to 259*259\n\nEncoder.eval()\nDecoder.eval()\n\nrandom_indexes = np.random.choice(range(0,len(y_testz)),128)\n\nx_testa = Variable(np.take(x_testz,random_indexes,0)).to(device)\n\noptimizer.zero_grad()\n\nlatent_variable = Encoder(x_testa)\nrecon_input = Decoder(latent_variable)\n#test_pic = recon_input[0].reshape([128,259])\n#plt.imshow(x_testa[0].reshape(128,259).detach().cpu())\n\ntest_pic = recon_input[0].reshape([-1,259,259])\nplt.imshow(x_testa[0].reshape(259,259).detach().cpu())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plt.imshow(test_pic.detach().cpu())\nplt.imshow(test_pic.reshape(259,259).detach().cpu())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from copy import deepcopy\n\ndef chunks(lst, n):\n    for i in range(0, len(lst), n):\n        yield lst[i:i + n]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Encoder.eval()\nDecoder.eval()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_features = []\nfor indx,ch in enumerate(chunks(x_trainz,512)):\n  X_train_batch = ch.to(device)\n  X_train_features_batch = Encoder(X_train_batch)\n  X_train_features.extend(X_train_features_batch)\n\n\nX_test_features = []\nfor ch in chunks(x_testz,512):\n  X_test_batch = ch.to(device)\n  X_test_features_batch = Encoder(X_test_batch)\n  X_test_features.extend(X_test_features_batch)\n\n\nX_train_features = torch.stack(X_train_features).detach().cpu().numpy()\nX_test_features = torch.stack(X_test_features).detach().cpu().numpy()\nX_train_features.shape, X_test_features.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(X_train_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(X_test_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# RUN THE SECOND AUTO MODEL\n\nEncoder2 = nn.Sequential(nn.Conv2d(1, 8, 3, stride=1, padding=0),\n                        nn.ReLU(),\n                        nn.Conv2d(8, 4, 3, stride=1, padding=0),\n                       )\n\n\nDecoder2 = nn.Sequential(nn.ConvTranspose2d(4, 8, 3, stride=1, padding=0),\n                        nn.ReLU(),\n                        nn.ConvTranspose2d(8, 1, 3, stride=1, padding=0),\n                        nn.Sigmoid()\n                       )\n\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nEncoder2 = Encoder2.to(device)\nDecoder2 = Decoder2.to(device)\n\n\noptimizer = optim.Adam(list(Encoder2.parameters()) + list(Decoder2.parameters()))\nloss_function = nn.MSELoss()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nrunning_loss = []\nfor iteration in range(10000**100):\n  \n  Encoder.train()\n  Decoder.train()\n\n  # # 1%, 25%, 50%, 75%, 100%\n  # #Fewer numbers of labels\n  percentage_of_label = .01\n  number_of_samples = int(len(y_trainz) * percentage_of_label)\n\n  random_indexes = np.random.choice(range(0,len(y_trainz)), number_of_samples)\n\n  #random_indexes = np.random.choice(range(0,len(y_trainz)),512)\n\n  x_traina = Variable(np.take(x_trainz,random_indexes,0)).to(device)\n  \n  optimizer.zero_grad()\n\n  latent_variable = Encoder2(x_traina)\n  recon_input = Decoder2(latent_variable)\n\n  loss = loss_function(recon_input, x_traina)\n\n  loss.backward()\n  \n  optimizer.step()\n\n  running_loss.append(loss.item())\n  \n\n\n  if iteration%1000 == 0:\n    r_loss = sum(running_loss)/len(running_loss)\n    print (r_loss)\n    running_loss = []\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Encoder2.eval()\nDecoder2.eval()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_features2 = []\nfor indx,ch in enumerate(chunks(x_trainz,512)):\n  X_train_batch = ch.to(device)\n  X_train_features_batch = Encoder2(X_train_batch)\n  X_train_features2.extend(X_train_features_batch)\n\n\nX_test_features2 = []\nfor ch in chunks(x_testz,512):\n  X_test_batch = ch.to(device)\n  X_test_features_batch = Encoder2(X_test_batch)\n  X_test_features2.extend(X_test_features_batch)\n\n\nX_train_features2 = torch.stack(X_train_features2).detach().cpu().numpy()\nX_test_features2 = torch.stack(X_test_features2).detach().cpu().numpy()\nX_train_features2.shape, X_test_features2.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reshaping forr the sake of clustering:\n\nX_train_features = X_train_features.reshape(864,4*255*255)\nX_test_features = X_test_features.reshape(288,4*255*255)\n\nX_train_features2 = X_train_features2.reshape(864,4*255*255)\nX_test_features2 = X_test_features2.reshape(288,4*255*255)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,15))\nplt.scatter(X_train_features2[:, 0], X_train_features2[:, 1], s= 5, c=y_trainz, cmap='Spectral')\nplt.gca().set_aspect('equal', 'datalim')\nplt.colorbar(boundaries=np.arange(11)-0.5).set_ticks(np.arange(8))\nplt.title('Training Data: Visualizing RAVDESS directly from autoencoder', fontsize=24);\nplt.xlabel('feature 1')\nplt.ylabel('feature 2')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,15))\nplt.scatter(X_test_features2[:, 0], X_test_features2[:, 1], s= 5, c=y_testz, cmap='Spectral')\nplt.gca().set_aspect('equal', 'datalim')\nplt.colorbar(boundaries=np.arange(11)-0.5).set_ticks(np.arange(8))\nplt.title('Testing Data: Visualizing RAVDESS directly from autoencoder', fontsize=24);\nplt.xlabel('feature 1')\nplt.ylabel('feature 2')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# PCA vs Autoencoder","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reshaping forr the sake of clustering:\n\nx_trainz = x_trainz.reshape(864,1*259*259)\nx_testz = x_testz.reshape(288,1*259*259)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#x_testz.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\npca = PCA(n_components=2)\npca_fit = pca.fit(x_trainz)\nprincipalComponents = pca_fit.transform(x_trainz) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,15))\nplt.scatter(principalComponents[:, 0], principalComponents[:, 1], s= 5, c=y_trainz, cmap='Spectral')\nplt.gca().set_aspect('equal', 'datalim')\nplt.colorbar(boundaries=np.arange(11)-0.5).set_ticks(np.arange(8))\nplt.title('Visualizing RAVDESS through PCA', fontsize=24);\nplt.xlabel('Principal Component 1')\nplt.ylabel('Principal Component 2')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,15))\nprincipalComponents = pca_fit.transform(x_testz)\nplt.scatter(principalComponents[:, 0], principalComponents[:, 1], s= 5, c=y_testz, cmap='Spectral')\nplt.gca().set_aspect('equal', 'datalim')\nplt.colorbar(boundaries=np.arange(11)-0.5).set_ticks(np.arange(8))\nplt.title('Visualizing RAVDESS through PCA', fontsize=24);\nplt.xlabel('Principal Component 1')\nplt.ylabel('Principal Component 2')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pca = PCA(n_components=2)\npca_fit = pca.fit(X_train_features)\nprincipalComponents = pca_fit.transform(X_train_features) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,15))\nplt.scatter(principalComponents[:, 0], principalComponents[:, 1], s= 5, c=y_trainz, cmap='Spectral')\nplt.gca().set_aspect('equal', 'datalim')\nplt.colorbar(boundaries=np.arange(11)-0.5).set_ticks(np.arange(8))\nplt.title('Visualizing RAVDESS through PCA', fontsize=24);\nplt.xlabel('Principal Component 1')\nplt.ylabel('Principal Component 2')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"principalComponents = pca_fit.transform(X_test_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,15))\nplt.scatter(principalComponents[:, 0], principalComponents[:, 1], s= 5, c=y_testz, cmap='Spectral')\nplt.gca().set_aspect('equal', 'datalim')\nplt.colorbar(boundaries=np.arange(11)-0.5).set_ticks(np.arange(8))\nplt.title('Visualizing RAVDESS through PCA', fontsize=24);\nplt.xlabel('Principal Component 1')\nplt.ylabel('Principal Component 2')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compare K means","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import KMeans","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pca = PCA(n_components=2)\nPCA_fit = pca.fit(x_trainz)\nPCA_train = pca.transform(x_trainz)\nPCA_test = pca.transform(x_testz)\nkmeans = KMeans(init=\"k-means++\", n_clusters=8, n_init=4)\nkmeans.fit(PCA_train)\nplt.figure(figsize=(20,15))\nplt.scatter(PCA_train[:, 0], PCA_train[:, 1], s= 5, c=y_trainz, cmap='Spectral')\ncentroids = kmeans.cluster_centers_\nplt.scatter(centroids[:, 0], centroids[:, 1], marker=\"o\", s=200, linewidths=5,\n            color=\"Black\", zorder=10)\n\n\nplt.gca().set_aspect('equal', 'datalim')\nplt.colorbar(boundaries=np.arange(11)-0.5).set_ticks(np.arange(8)) # Maybe this should be 8??\nplt.title('Visualizing RAVDESS through PCA', fontsize=24);\nplt.xlabel('Principal Component 1')\nplt.ylabel('Principal Component 2')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,15))\nplt.scatter(PCA_test[:, 0], PCA_test[:, 1], s= 5, c=y_testz, cmap='Spectral')\ncentroids = kmeans.cluster_centers_\nplt.scatter(centroids[:, 0], centroids[:, 1], marker=\"o\", s=200, linewidths=5,\n            color=\"Black\", zorder=10)\n\n\nplt.gca().set_aspect('equal', 'datalim')\nplt.colorbar(boundaries=np.arange(11)-0.5).set_ticks(np.arange(8))\nplt.title('Visualizing RAVDESS through PCA', fontsize=24);\nplt.xlabel('Principal Component 1')\nplt.ylabel('Principal Component 2')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pca = PCA(n_components=2)\nPCA_fit = pca.fit(X_train_features)\nPCA_train = pca.transform(X_train_features)\nPCA_test = pca.transform(X_test_features)\nkmeans = KMeans(init=\"k-means++\", n_clusters=8, n_init=4) #Changed to 8 clusters\nkmeans.fit(PCA_train)\nplt.figure(figsize=(20,15))\nplt.scatter(PCA_train[:, 0], PCA_train[:, 1], s= 5, c=y_trainz, cmap='Spectral')\ncentroids = kmeans.cluster_centers_\nplt.scatter(centroids[:, 0], centroids[:, 1], marker=\"o\", s=200, linewidths=5,\n            color=\"Black\", zorder=10)\n\n\nplt.gca().set_aspect('equal', 'datalim')\nplt.colorbar(boundaries=np.arange(11)-0.5).set_ticks(np.arange(8))\nplt.title('Visualizing RAVDESS through PCA', fontsize=24);\nplt.xlabel('Principal Component 1')\nplt.ylabel('Principal Component 2')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,15))\nplt.scatter(PCA_test[:, 0], PCA_test[:, 1], s= 5, c=y_testz, cmap='Spectral')\ncentroids = kmeans.cluster_centers_\nplt.scatter(centroids[:, 0], centroids[:, 1], marker=\"o\", s=200, linewidths=5,\n            color=\"Black\", zorder=10)\n\n\nplt.gca().set_aspect('equal', 'datalim')\nplt.colorbar(boundaries=np.arange(11)-0.5).set_ticks(np.arange(8))\nplt.title('Visualizing RAVDESS through PCA', fontsize=24);\nplt.xlabel('Principal Component 1')\nplt.ylabel('Principal Component 2')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fit a Classifier on the Representation with fewer label data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Fewer numbers of labels\n#percentage_of_label = .10\npercentage_of_label = .01\nnumber_of_samples = int(len(y_trainz) * percentage_of_label)\n\nrandom_indexes = np.random.choice(range(0,len(y_trainz)), number_of_samples)\nsmall_x_train = np.take(x_trainz,random_indexes,0)\nsmall_y_train = np.take(y_trainz,random_indexes,0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# PCA version!!!","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pca = PCA(n_components=8) # ??? Check to make sure this number is correct, it should probably be 8. Or perhaps not. It was 16\nPCA_fit = pca.fit(small_x_train)\nPCA_train = pca.transform(small_x_train)\nPCA_test = pca.transform(x_testz)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_reg = RandomForestClassifier(random_state=42)\nrf_reg.fit(PCA_train, small_y_train)\nprint(\"Train Accuracy: \", rf_reg.score(PCA_train, small_y_train))\nprint(\"Test Accuracy: \", rf_reg.score(PCA_test,y_testz))\n#\ny_pred = rf_reg.predict(PCA_test)\nprint(\"F1 Score: \", f1_score(y_testz, y_pred, average='macro'))\nprint(\"Recall: \", recall_score(y_testz, y_pred, average='macro'))\nprint(\"Precision: \", precision_score(y_testz, y_pred, average='macro'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Encoder 1 version!!!","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# #Fewer numbers of labels\n# percentage_of_label = .01\n# number_of_samples = int(len(train_y) * percentage_of_label)\n\n# random_indexes = np.random.choice(range(0,len(train_y)), number_of_samples)\nsmall_x_train = np.take(X_train_features,random_indexes,0)\nsmall_y_train = np.take(y_trainz,random_indexes,0)\nrf_reg = RandomForestClassifier(random_state=42)\nrf_reg.fit(small_x_train, small_y_train)\nprint(\"Train Accuracy: \", rf_reg.score(small_x_train, small_y_train))\nprint(\"Test Accuracy: \", rf_reg.score(X_test_features,y_testz))\n#\ny_pred = rf_reg.predict(X_test_features)\nprint(\"F1 Score: \", f1_score(y_testz, y_pred, average='macro'))\nprint(\"Recall: \", recall_score(y_testz, y_pred, average='macro'))\nprint(\"Precision: \", precision_score(y_testz, y_pred, average='macro'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Encoder 2 version!!!","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # 1%, 25%, 50%, 75%, 100%\n# #Fewer numbers of labels\n# percentage_of_label = .01\n# number_of_samples = int(len(train_y) * percentage_of_label)\n\n# random_indexes = np.random.choice(range(0,len(train_y)), number_of_samples)\n\n\n\n\n#X_train_features2 = X_train_features2.reshape(864,4*255*255)\n#X_test_features2 = X_test_features2.reshape(288,4*255*255)\n\nsmall_x_train = np.take(X_train_features2,random_indexes,0)\nsmall_y_train = np.take(y_trainz,random_indexes,0)\nrf_reg = RandomForestClassifier(random_state=42)\n\n#small_x_train = small_x_train.reshape(512,4*255*255)\n\n\n\nrf_reg.fit(small_x_train, small_y_train)\n\n\nprint(\"Train Accuracy: \", rf_reg.score(small_x_train, small_y_train))\nprint(\"Test Accuracy: \", rf_reg.score(X_test_features2,y_testz))\n#\ny_pred = rf_reg.predict(X_test_features2)\nprint(\"F1 Score: \", f1_score(y_testz, y_pred, average='macro'))\nprint(\"Recall: \", recall_score(y_testz, y_pred, average='macro'))\nprint(\"Precision: \", precision_score(y_testz, y_pred, average='macro'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Do 100% now!!!!!!!!!!!!!!!!!!!!","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 1%, 25%, 50%, 75%, 100%","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}